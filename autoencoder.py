# -*- coding: utf-8 -*-
"""Autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/115UiLQTfaa2BTX8j-ugPkVFgQpe_O3cU
"""

import numpy as np
import random
from __future__ import print_function
import torch
from torchvision import transforms, utils, datasets
train_on_gpu = torch.cuda.is_available()
if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import PIL
from collections import Counter

train_data = datasets.MNIST(root='./data', train=True,download=True)
X_train = train_data.data
data_t = np.array(X_train[:64].reshape(64, -1))
data_t = (data_t-data_t.min(axis=1).reshape(64,1))/(data_t.max(axis=1).reshape(64,1)-data_t.min(axis=1).reshape(64,1))

class sigmoid_activation:  
  def sigmoid(self,x):
    return 1. / (1 + np.exp(-x))
  def sigmoid_derivative(self,x):
    sig = 1. / (1 + np.exp(-x))
    return sig*(1-sig)

class Relu:
  def relu(self, x):
    out = x
    out[x < 0] = 0
    return out
  def relu_derivative(self, x):
    x[x<=0]=0
    x[x>0]=1
    return x

class Autoencoder:
  def __init__(self, n_input, n_hidden):
    self.n_input = n_input
    self.n_hidden = n_hidden
    np_rng = np.random.RandomState(123)
    self.W_1 = np.asarray(np_rng.uniform(
      low=-0.1 * np.sqrt(6. / (n_hidden + n_input)),
                        high=0.1 * np.sqrt(6. / (n_hidden + n_input)),
                        size=(n_input, n_hidden)))
    self.W_2 = np.asarray(np_rng.uniform(
      low=-0.1 * np.sqrt(6. / (n_hidden + n_input)),
                        high=0.1 * np.sqrt(6. / (n_hidden + n_input)),
                        size=(n_hidden, n_input)))

    ## forward 
  def forward_(self, train_data, lr=0.01):
    hidden_1 = np.dot(train_data, self.W_1)
    activation_hidden = sigmoid_activation().sigmoid(hidden_1)
    output_1 = np.dot(activation_hidden, self.W_2)
    output = sigmoid_activation().sigmoid(output_1)
    loss = np.sum((output-train_data)**2)/train_data.shape[0]
    ##backpropagation
    error_2 = np.multiply((output-train_data), sigmoid_activation().sigmoid_derivative(output_1))
    grad_w2 = np.dot(activation_hidden.T, error_2)

    error_1 = np.multiply(np.dot(error_2,self.W_2.T), sigmoid_activation().sigmoid_derivative(hidden_1))
    grad_W1 = np.dot(train_data.T,error_1)

    self.W_1 = self.W_1 - lr*grad_W1
    self.W_2 = self.W_2 - lr*grad_w2
    return loss

  def hidden_layer(self, data):
    return sigmoid_activation().sigmoid(np.dot(data, self.W_1))

batch_size = 60
data1 = np.array(X_train).reshape(len(X_train), -1) 
data1 = (data1-data1.min(axis=1).reshape(60000,1))/(data1.max(axis=1).reshape(60000,1)-data1.min(axis=1).reshape(60000,1))
length = int(len(data1)/batch_size)
autoencoder = Autoencoder(n_input=784, n_hidden=25)
epochs=250
for epoch in range(epochs):
  epoch_loss = 0
  for i in range(length):
    data_1 = data1[batch_size*i: batch_size*(i+1)]
    loss = autoencoder.forward_(data_1, lr=0.1)
    epoch_loss += loss
  epoch_loss = epoch_loss/length
  print("epoch %s train_loss %s" % (epoch, epoch_loss))

cluster_data = np.array(X_train[:100].reshape(100, -1))
ouput_data = autoencoder.hidden_layer(cluster_data)

kmeans = KMeans(n_clusters=10, random_state=42)
kmeans.fit_transform(ouput_data)
labels = list(kmeans.labels_)
index_dict = {}
for label in labels:
  index_dict[label] = [i for i, x in enumerate(labels) if x == label]

def print_clusters(label, dict_, y_train):
  labels_ = dict_[label]
  plt.figure(figsize=(20,20))
  k = len(labels_)
  sorted_x = dict(Counter(list(np.array(y_train[labels_]))))
  sorted_x = dict(sorted(sorted_x.items(), key=lambda kv: kv[1], reverse=True))
  labels_1 = ''
  percentage = ''
  j = 1
  for k, v in sorted_x.items():
    labels_1 = labels_1 +'  ' + str(k)
    percentage = percentage + ' '+ str(v*100/sum(sorted_x.values()))[:2] + ' %'
    j = j+1
  for i, idx in enumerate(labels_):
    plt.subplot(k+1, len(labels_), i+1)
    plt.imshow(np.array(X_train[idx]))
  plt.title('Labels  ' + str(labels_1) + '   Percentage   ' + str(percentage))

Y_train = train_data.train_labels

print_clusters(0, index_dict, Y_train)











