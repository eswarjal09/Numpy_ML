# -*- coding: utf-8 -*-
"""Restricted Boltzmann machines

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fAAfAnq8O7xlQ9UZ08aStGSOZNH1hDh2
"""

import numpy as np
import random
from __future__ import print_function
import torch
from torchvision import transforms, utils, datasets
train_on_gpu = torch.cuda.is_available()
if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')
from keras.models import Sequential
from keras.layers import UpSampling2D, Conv2DTranspose
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import PIL
from collections import Counter

train_data = datasets.MNIST(root='./data', train=True,download=True)
X_train = train_data.data

Y_train = train_data.train_labels

class RBM:
  def __init__(self, num_hidden, num_visible):
    self.num_hidden = num_hidden
    self.num_visible = num_visible
    np_rng = np.random.RandomState(1234)
    #self.weights = np.array(np.random.uniform(0,1, size=(num_visible, num_hidden)))
    self.weights = np.asarray(np_rng.uniform(
			low=-0.1 * np.sqrt(6. / (num_hidden + num_visible)),
                       	high=0.1 * np.sqrt(6. / (num_hidden + num_visible)),
                       	size=(num_visible, num_hidden)))
    self.weights = np.insert(self.weights, 0, 0, axis = 0)
    self.weights = np.insert(self.weights, 0, 0, axis = 1)

  def logistic(self,x):
    return 1/(1+np.exp(-x))
  def train(self, data,learning_rate=0.1):
    num_examples = data.shape[0]
    # Insert bias units of 1 into the first column.
    data = np.insert(data, 0, 1, axis = 1)
    ## calculate hidden layer 
    hidden_v = np.dot(data, self.weights)
    ## activation for hidden layer
    hidden_probs = self.logistic(hidden_v)
    ## adjust bias
    hidden_probs[:, 0] = 1
    ## Randomly shutdowm hidden neurons
    hidden_probs_act = hidden_probs > np.random.rand(num_examples, self.num_hidden+1)
    #try with hidden_probs_act too 
    ## calculate positive association using data and hidden activations
    energy_pos = np.dot(data.T, hidden_probs)
    ## calculate visible neurons z
    visible_v = np.dot(hidden_probs_act, self.weights.T)
    ## calculate visible activation
    visible_probs = self.logistic(visible_v)
    ##adjust visible layers bias  
    visible_probs[:, 0] = 1
    #hidden layer z with new visible layer weights and weights
    neg_hidden_v = np.dot(visible_probs, self.weights)
    ## activations for the hidden layer
    neg_hidden_probs = self.logistic(neg_hidden_v)
    ## calculate negative association between visible and hidden 
    energy_neg = np.dot(visible_probs.T, neg_hidden_probs)
    ## update weights after one sequence
    self.weights += learning_rate*((energy_pos-energy_neg)/num_examples)

    error = np.sum((data - visible_probs)**2)/num_examples
    return error

  def transform(self, input_data):
    input_data = np.insert(input_data, 0, 1, axis = 1)
    outputs = np.dot(input_data, self.weights)
    outputs_activation = self.logistic(outputs)
    return outputs_activation

epochs =100
batch_size = 120
n_batches = int(len(X_train)/batch_size)
rbm = RBM(num_hidden = 25, num_visible=784)
for epoch in range(epochs):
  error_epoch = 0
  err = 0
  for i in range(n_batches):
    data = np.array(X_train[batch_size*(i) : batch_size*(i+1)]).reshape(batch_size, -1)
    data= data/data.max(axis=1).reshape(data.shape[0],1)
    #data = np.where(data< 225, 0, 1)
    error = rbm.train(data, learning_rate=0.01)
    err += error
  error_epoch = err/n_batches
  print("epoch %s error is : %s " % (epoch, error_epoch))

cluster_data = np.array(X_train[:100].reshape(100, -1))
ouput_data = rbm.transform(cluster_data)

kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit_transform(ouput_data)
labels = list(kmeans.labels_)

index_dict = {}
for label in labels:
  index_dict[label] = [i for i, x in enumerate(labels) if x == label]

def print_clusters(label, dict_, y_train):
  labels_ = dict_[label]
  plt.figure(figsize=(20,20))
  k = len(labels_)
  sorted_x = dict(Counter(list(np.array(y_train[labels_]))))
  sorted_x = dict(sorted(sorted_x.items(), key=lambda kv: kv[1], reverse=True))
  labels_1 = ''
  percentage = ''
  j = 1
  for k, v in sorted_x.items():
    labels_1 = labels_1 +'  ' + str(k)
    percentage = percentage + ' '+ str(v*100/sum(sorted_x.values()))[:2] + ' %'
    j = j+1
  for i, idx in enumerate(labels_):
    plt.subplot(k+1, len(labels_), i+1)
    plt.imshow(np.array(X_train[idx]))
  plt.title('Labels  ' + str(labels_1) + '   Percentage   ' + str(percentage))

print_clusters(0, index_dict, Y_train)

print_clusters(1, index_dict, Y_train)

print_clusters(2, index_dict, Y_train)

print_clusters(3, index_dict, Y_train)

print_clusters(4, index_dict, Y_train)

print_clusters(5, index_dict, Y_train)

print_clusters(6, index_dict, Y_train)

print_clusters(7, index_dict, Y_train)

print_clusters(8, index_dict, Y_train)

print_clusters(9, index_dict, Y_train)



# example of using the upsampling layer

# define input data
X = np.asarray([[1, 2],
			 [3, 4]])
# show input data for context
print(X)
# reshape input data into one sample a sample with a channel
X = X.reshape((1, 2, 2, 1))
# define model
model = Sequential()
model.add(UpSampling2D(input_shape=(2, 2, 1)))
# summarize the model
model.summary()
# make a prediction with the model
yhat = model.predict(X)
# reshape output to remove channel to make printing easier
yhat = yhat.reshape((4, 4))
# summarize output
print(yhat)

X = np.asarray([[1, 2],
			 [3, 4]])
# show input data for context
print(X)
# reshape input data into one sample a sample with a channel
X = X.reshape((1, 2, 2, 1))
# define model
model = Sequential()
model.add(Conv2DTranspose(1, (1,1), strides=(2,2), input_shape=(2, 2, 1)))
# summarize the model
model.summary()
# define weights that they do nothing
weights = [np.asarray([[[[1]]]]), np.asarray([0])]
# store the weights in the model
model.set_weights(weights)
# make a prediction with the model
yhat = model.predict(X)
# reshape output to remove channel to make printing easier
yhat = yhat.reshape((4, 4))
# summarize output
print(yhat)

weights[0].shape

